{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 1 创建张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和NumPy中的dnarray一样，张量的本质也是结构化的组织了大量的数据。并且，在实际操作过程中，张量的创建和基本功能也和NumPy中的array非常类似。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 创建张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1+cpu'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([1, 2]) # 通过列表创建张量\n",
    "t = torch.tensor((1, 2)) # 通过元组创建张量\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2], dtype=torch.int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array((1, 2)) # 通过数组创建张量\n",
    "t1 = torch.tensor(a)\n",
    "t1 # 通过上述返回结果，我们发现数组的张量也有dtype类型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 张量类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整数型的数组默认创建int32（整型）类型，而张量则默认创建int64（长整型）类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int32"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dtype # 数组类型 dtype('int32')\n",
    "t.dtype  # torch.int64\n",
    "t1.dtype # torch.int32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相对的，创建浮点型数组时，张量默认是float32（单精度浮点型），而Array则是默认float64（双精度浮点型）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1.1, 2.2]).dtype # dtype('float64')\n",
    "torch.tensor(np.array([1.1, 2.2])).dtype # torch.float64\n",
    "torch.tensor([1.11, 2.2]).dtype # torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了数值型张量，常用的常量类型还有布尔型张量，也就是构成张量的各元素都是布尔类型的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bool"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.tensor([True, False]) \n",
    "t2.dtype # torch.bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center>PyTorch中Tensor类型</center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|数据类型|dtype|      \n",
    "|:--:|:--:|      \n",
    "|32bit浮点数|torch.float32或torch.float|      \n",
    "|64bit浮点数|torch.float64或torch.double|\t      \n",
    "|16bit浮点数|torch.float16或torch.half|\t      \n",
    "|8bit无符号整数|torch.unit8|    \n",
    "|8bit有符号整数|torch.int8|\n",
    "|16bit有符号整数|torch.int16或torch.short|\n",
    "|16bit有符号整数|torch.int16或torch.short|\n",
    "|32bit有符号整数|torch.int32或torch.int|\n",
    "|64bit有符号整数|torch.int64或torch.long|\n",
    "|布尔型|torch.bool|\n",
    "|复数型|torch.complex64|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，我们还可以通过dtype参数，在创建张量过程中设置输出结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2], dtype=torch.int16)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建int16整型张量\n",
    "torch.tensor([1.1, 2.7], dtype = torch.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，在PyTorch中也支持复数类型对象创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.+2.j)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(1 + 2j)           # 1是实部、2是虚部\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 张量类型转化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 张量类型的隐式转化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和NumPy中array相同，当张量各元素属于不同类型时，系统会自动进行隐式转化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 浮点型和整数型的隐式转化\n",
    "torch.tensor([1.1, 2]).dtype "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 布尔型和数值型的隐式转化\n",
    "torch.tensor([True, 2.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 张量类型的转化方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，我们还可以使用.float()、.int()等方法对张量类型进行转化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转化为默认浮点型（32位）\n",
    "t.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.], dtype=torch.float64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转化为双精度浮点型\n",
    "t.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2], dtype=torch.int16)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转化为16位整数\n",
    "t.short()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 2 张量的维度与形变"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量作为一组数的结构化表示，也同样拥有维度的概念。简单理解，向量就是一维的数组，而矩阵则是二维的数组，以此类推，在张量中，我们还可以定义更高维度的数组。当然，张量的高维数组和NumPy中高维Array概念类似。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## 2.1 张量的维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 1维张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和NumPy不同，PyTorch中size方法返回结果和shape属性返回结果一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([1, 2])\n",
    "\n",
    "t1.ndim # 使用ndim属性查看张量的维度 1\n",
    "t1.shape # 使用shape查看形状 torch.Size([2])\n",
    "t1.size() # 和size函数相同 torch.Size([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2])\n",
    "\n",
    "a.shape # (2,)\n",
    "a.size # 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，还需要注意有两个常用的函数/方法，用来查看张量的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t1) # 返回拥有几个（N-1）维元素 2\n",
    "t1.numel() # # 返回总共拥有几个数 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 2维张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用list的list创建二维数组\n",
    "t2 = torch.tensor([[1, 2], [3, 4]])\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.shape             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t2) # 此处len函数返回结果代表t2由两个1维张量构成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.numel() # 此处numel方法返回结果代表t2由总共由4个数构成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 零维张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在PyTorch中，还有一类特殊的张量，被称为零维张量。该类型张量只包含一个元素，但又不是单独一个数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前，我们可将零维张量视为拥有张量属性的单独一个数。（例如，张量可以存在GPU上，但Python原生的数值型对象不行，但零维张量可以，尽管是零维。）从学术名称来说，Python中单独一个数是scalars（标量），而零维的张量则是tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor(1)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 高维张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般来说，三维及三维以上的张量，我们就将其称为高维张量。当然，在高维张量中，最常见的还是三维张量。我们可以将其理解为二维数组或者矩阵的集合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 2],\n",
       "       [3, 4, 4]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = np.array([[1, 2, 2], [3, 4, 4]])\n",
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 6, 6],\n",
       "       [7, 8, 8]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2 = np.array([[5, 6, 6], [7, 8, 8]])\n",
    "a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 2],\n",
       "         [3, 4, 4]],\n",
       "\n",
       "        [[5, 6, 6],\n",
       "         [7, 8, 8]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 由两个形状相同的二维数组创建一个三维的张量\n",
    "t3 = torch.tensor([a1, a2])\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3.shape   # 包含两个，两行三列的矩阵的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 张量的形变"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量作为数字的结构化集合，其结构也是可以根据实际需求灵活调整的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 拉平"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 2],\n",
       "         [3, 4, 4]],\n",
       "\n",
       "        [[5, 6, 6],\n",
       "         [7, 8, 8]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 2, 3, 4, 4, 5, 6, 6, 7, 8, 8], dtype=torch.int32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果将零维张量使用flatten，则会将其转化为一维张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.flatten().ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 任意变形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转化为两行、一列的向量\n",
    "t1.reshape(2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转化后生成一维张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转化后生成一维张量\n",
    "t1.reshape(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.reshape(2).ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 注，另一种表达形式\n",
    "t1.reshape(2, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转化后生成二维张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.reshape(1, 2)        # 生成包含一个两个元素的二维张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转化后生成三维张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2]]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.reshape(1, 1, 2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [2]]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.reshape(1, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 注意转化过程维度的变化\n",
    "t1.reshape(1, 2, 1).ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 特殊张量和形状创建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 特殊张量创建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全0张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros([2, 3])            # 创建全是0的，两行、三列的张量（矩阵）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全1张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones([2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单位矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对角矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "略有特殊的是，在PyTorch中，需要利用一维张量去创建对角矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "diag(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13288/4008606110.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m             \u001b[1;31m# 不能使用list直接创建对角矩阵\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: diag(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "torch.diag([1, 2])             # 不能使用list直接创建对角矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0],\n",
       "        [0, 2]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rand：服从0-1均匀分布的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3145, 0.4227, 0.9461],\n",
       "        [0.7049, 0.1644, 0.7239]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "randn：服从标准正态分布的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6820, -0.4783,  0.6096],\n",
       "        [-0.3581,  0.6998, -0.7130]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normal：服从指定正态分布的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8881, 1.2751],\n",
       "        [2.8970, 5.5043]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(2, 3, size = (2, 2))  # 均值为2，标准差为3的张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "randint：整数随机采样结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 8, 6, 1],\n",
       "        [6, 5, 3, 4]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(1, 10, [2, 4])   # 在1-10之间随机抽取整数，组成两行四列的矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arange/linspace：生成数列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(5) # 和range相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.5000, 2.0000, 2.5000, 3.0000, 3.5000, 4.0000, 4.5000])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1, 5, 0.5)  # 从1到5（左闭右开），每隔0.5取值一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 3., 5.])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(1, 5, 3)  # 从1到5（左右都包含），等距取三个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "empty：生成未初始化的指定形状矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3399e-08, 1.3096e-11, 8.4947e+20],\n",
       "        [2.7449e-06, 2.0971e-07, 7.9869e+20]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "full：根据指定形状，填充指定数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 2, 2],\n",
       "        [2, 2, 2, 2]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full([2, 4], 2)                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 指定形状创建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，我们还能根据指定对象的形状进行数值填充，只需要在上述函数后面加上_like即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full_like(t1, 2)  # 根据t1形状，填充数值2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9, 4],\n",
       "        [9, 9]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint_like(t2, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros_like(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意一点的是，_like类型转化需要注意转化前后数据类型一致的问题；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"normal_kernel_cpu\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13288/1629091467.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# t1是整数，而转化后将变为浮点数，此时代码将报错\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: \"normal_kernel_cpu\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "torch.randn_like(t1)  # t1是整数，而转化后将变为浮点数，此时代码将报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1000, 2.2000])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t10 = torch.tensor([1.1, 2.2])   # 重新生成一个新的浮点型张量\n",
    "t10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0115, -0.2987])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn_like(t10)   # 即可执行相应的填充转化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 3 张量类型转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量、数组和列表是较为相似的三种类型对象，在实际操作过程中，经常会涉及三种对象的相互转化。在此前张量的创建过程中，我们看到torch.tensor函数可以直接将数组或者列表转化为张量，而我们也可以将张量转化为数组或者列表。另外，前文介绍了0维张量的概念，此处也将进一步给出零维张量和数值对象的转化方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2], dtype=int64)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.numpy() # .numpy方法：张量转化为数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2], dtype=int64)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 当然，也可以通过np.array函数直接转化为array\n",
    "np.array(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的是，此时转化的列表是由一个个零维张量构成的列表，而非张量的数值组成的列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1), tensor(2)]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在很多情况下，我们需要将最终计算的结果张量转化为单独的数值进行输出，此时需要使用.item方法来执行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = torch.tensor(1)\n",
    "n.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 4 张量的深拷贝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python中其他对象类型一样，等号赋值操作实际上是浅拷贝，需要进行深拷贝，则需要使用clone方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t11 = t1       # t11是t1的浅拷贝       \n",
    "t11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[1]                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1[1] = 10     # t1修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 10])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 10])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t11     # t11会同步修改"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处t1和t11二者指向相同的对象。而要使得t11不随t1对象改变而改变，则需要对t11进行深拷贝，从而使得t11单独拥有一份对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "t11 = t1.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 10])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 10])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100,  10])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[0] = 100\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 10])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 5 张量的索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而作为PyTorch中基本数据类型，张量即具备了列表、数组的基本功能，同时还充当着向量、矩阵、甚至是数据框等重要数据结构，因此PyTorch中也设置了非常完备的张量合并与变换的操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## 5.1 张量的符号索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量也是有序序列，我们可以根据每个元素在系统内的顺序“编号”，来找出特定的元素，也就是索引。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 1维张量索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一维张量的索引过程和Python原生对象类型的索引一致，基本格式遵循[start: end: step]，索引的基本要点回顾如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.arange(1, 11)\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：张量索引出来的结果还是零维张量， 而不是单独的数。要转化成单独的数，需要使用item()方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "冒号分隔，表示对某个区域进行索引，也就是所谓的切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[1: 8]   # 索引其中2-9号元素，并且左包含右不包含"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个冒号，表示索引的间隔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 6, 8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[1: 8: 2]   # 索引其中2-9号元素，左包含右不包含，且隔两个数取一个"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "冒号前后没有值，表示索引这个区域"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  4,  6,  8, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[1: : 2]  # 从第二个元素开始索引，一直到结尾，并且每隔两个数取一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5, 7])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[: 8: 2] # 从第一个元素开始索引到第9个元素（不包含），并且每隔两个数取一个"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在张量的索引中，step位必须大于0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "step must be greater than zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10876/529333786.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mt1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: step must be greater than zero"
     ]
    }
   ],
   "source": [
    "t1[9: 1: -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 2维张量索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二维张量的索引逻辑和一维张量的索引逻辑基本相同，二维张量可以视为两个一维张量组合而成，而在实际的索引过程中，需要用逗号进行分隔，分别表示对哪个一维张量进行索引、以及具体的一维张量的索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.arange(1, 10).reshape(3, 3)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2[0, 1]  # 表示索引第一行、第二个（第二列的）元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2[0, ::2]  # 表示索引第一行、每隔两个元素取一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2[0, [0, 2]]  # 索引结果同上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 3],\n",
       "        [7, 9]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2[::2, ::2]  # 表示每隔两行取一行、并且每一行中每隔两个元素取一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 8])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2[[0, 2], 1] # 索引第一行、第三行、第二列的元素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对二维张量来说，基本可以视为是对矩阵的索引，并且行、列的索引遵照相同的索引规范，并用逗号进行分隔。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 3维张量的索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在二维张量索引的基础上，三维张量拥有三个索引的维度。我们将三维张量视作矩阵组成的序列，则在实际索引过程中拥有三个维度，分别是索引矩阵、索引矩阵的行、索引矩阵的列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3],\n",
       "         [ 4,  5,  6],\n",
       "         [ 7,  8,  9]],\n",
       "\n",
       "        [[10, 11, 12],\n",
       "         [13, 14, 15],\n",
       "         [16, 17, 18]],\n",
       "\n",
       "        [[19, 20, 21],\n",
       "         [22, 23, 24],\n",
       "         [25, 26, 27]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = torch.arange(1, 28).reshape(3, 3, 3)\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3[1, 1, 1] # 索引第二个矩阵中，第二行、第二个元素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "索引本质上是先外面再里面的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3[0, 1, 2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 12],\n",
       "        [16, 18]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3[1, ::2, ::2]  # 索引第二个矩阵，行和列都是每隔两个取一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  3],\n",
       "         [ 7,  9]],\n",
       "\n",
       "        [[19, 21],\n",
       "         [25, 27]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3[:: 2, :: 2, :: 2] # 每隔两个取一个矩阵，对于每个矩阵来说，行和列都是每隔两个取一个"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更为本质的角度去理解高维张量的索引，其实就是围绕张量的“形状”进行索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3[1, 1, 1]             # 与shape一一对应"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 张量的函数索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在PyTorch中，我们还可以使用index_select函数，通过指定index来对张量进行索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.tensor([1, 2])\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.index_select(t1, 0, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在index_select函数中，第二个参数实际上代表的是索引的维度。对于t1这个一维向量来说，由于只有一个维度，因此第二个参数取值为0，就代表在第一个维度上进行索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8],\n",
       "        [ 9, 10, 11]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.arange(12).reshape(4, 3)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 4, 5],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.index_select(t2, 0, indices)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 4, 5],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.index_select(t2, 0, indices)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dim参数取值为0，代表在shape的第一个维度上索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2],\n",
       "        [ 4,  5],\n",
       "        [ 7,  8],\n",
       "        [10, 11]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.index_select(t2, 1, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dim参数取值为1，代表在shape的第一个维度上索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dim参数取值为0，代表在shape的第一个维度上索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2],\n",
       "        [ 4,  5],\n",
       "        [ 7,  8],\n",
       "        [10, 11]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.index_select(t2, 1, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dim参数取值为1，代表在shape的第一个维度上索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 6 张量的分片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 张量的视图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在正式介绍张量的切分方法之前，需要首先介绍PyTorch中的.view()方法。该方法会返回一个类似视图的结果，该结果和原张量对象共享一块数据存储空间，并且通过.view()方法，还可以改变对象结构，生成一个不同结构，但共享一个存储空间的张量。当然，共享一个存储空间，也就代表二者是“浅拷贝”的关系，修改其中一个，另一个也会同步进行更改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(6).reshape(2, 3)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te = t.view(3, 2)   # 构建一个数据相同，但形状不同的“视图”\n",
    "te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[0] = 1                       # 对t进行修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [1, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te                             # te同步变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1],\n",
       "         [3, 4, 5]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = t.view(1, 2, 3)           # 维度也可以修改\n",
    "tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“视图”的作用就是节省空间，而值得注意的是，在接下来介绍的很多切分张量的方法中，返回结果都是“视图”，而不是新生成一个对象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 chunk分块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chunk函数能够按照某维度，对张量进行均匀切分，并且返回结果是原张量的视图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8],\n",
       "        [ 9, 10, 11]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.arange(12).reshape(4, 3)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2]]),\n",
       " tensor([[3, 4, 5]]),\n",
       " tensor([[6, 7, 8]]),\n",
       " tensor([[ 9, 10, 11]]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc = torch.chunk(t2, 4, dim=0)   # 在第零个维度上（按行），进行四等分\n",
    "tc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：chunk返回结果是一个视图，不是新生成了一个对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc[0][0][0] = 1            # 修改tc中的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 1, 2]]),\n",
       " tensor([[3, 4, 5]]),\n",
       " tensor([[6, 7, 8]]),\n",
       " tensor([[ 9, 10, 11]]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8],\n",
       "        [ 9, 10, 11]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2                         # 原张量也会对应发生变化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当原张量不能均分时，chunk不会报错，但会返回其他均分的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 1, 2],\n",
       "         [3, 4, 5]]),\n",
       " tensor([[ 6,  7,  8],\n",
       "         [ 9, 10, 11]]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.chunk(t2, 3, dim=0)  # 次一级均分结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(torch.chunk(t2, 3, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 1, 2]]),\n",
       " tensor([[3, 4, 5]]),\n",
       " tensor([[6, 7, 8]]),\n",
       " tensor([[ 9, 10, 11]]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.chunk(t2, 5, dim=0)            # 次一级均分结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 split切分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split既能进行均分，也能进行自定义切分。当然，需要注意的是，和chunk函数一样，split返回结果也是view。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8],\n",
       "        [ 9, 10, 11]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.arange(12).reshape(4, 3)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2],\n",
       "         [3, 4, 5]]),\n",
       " tensor([[ 6,  7,  8],\n",
       "         [ 9, 10, 11]]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(t2, 2, 0)  # 第二个参数只输入一个数值时表示均分，第三个参数表示切分的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2]]),\n",
       " tensor([[ 3,  4,  5],\n",
       "         [ 6,  7,  8],\n",
       "         [ 9, 10, 11]]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(t2, [1, 3], 0)  # 第二个参数输入一个序列时，表示按照序列数值进行切分，也就是1/3分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**，当第二个参数位输入一个序列时，序列的各数值的和必须等于对应维度下形状分量的取值。例如，上述代码中，是按照第一个维度进行切分，而t2总共有4行，因此序列的求和必须等于4，也就是1+3=4，而序列中每个分量的取值，则代表切块大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2]]),\n",
       " tensor([[3, 4, 5]]),\n",
       " tensor([[6, 7, 8]]),\n",
       " tensor([[ 9, 10, 11]]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(t2, [1, 1, 1, 1], 0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2]]),\n",
       " tensor([[3, 4, 5]]),\n",
       " tensor([[ 6,  7,  8],\n",
       "         [ 9, 10, 11]]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(t2, [1, 1, 2], 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [3],\n",
       "         [6],\n",
       "         [9]]),\n",
       " tensor([[ 1,  2],\n",
       "         [ 4,  5],\n",
       "         [ 7,  8],\n",
       "         [10, 11]]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = torch.split(t2, [1, 2], 1) \n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts[0][0] = 1                 # view进行修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1],\n",
       "         [3],\n",
       "         [6],\n",
       "         [9]]),\n",
       " tensor([[ 1,  2],\n",
       "         [ 4,  5],\n",
       "         [ 7,  8],\n",
       "         [10, 11]]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8],\n",
       "        [ 9, 10, 11]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2                           # 原对象同步改变"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor的split方法和array的split方法有很大的区别，array的split方法是根据索引进行切分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 7 张量的合并"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量的合并操作类似与列表的追加元素，可以拼接、也可以堆叠。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 cat拼接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch中，可以使用cat函数实现张量的拼接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros(2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.ones(2, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.zeros(3, 3)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a, b]) # 按照行进行拼接，dim默认取值为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1.]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a, b], 1)  # 按照列进行拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Got 2 and 3 in dimension 0 (The offending index is 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10876/1470550129.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 形状不匹配时将报错\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Got 2 and 3 in dimension 0 (The offending index is 1)"
     ]
    }
   ],
   "source": [
    "torch.cat([a, c], 1)  # 形状不匹配时将报错"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意理解，拼接的本质是实现元素的堆积，也就是构成a、b两个二维张量的各一维张量的堆积，最终还是构成二维向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 stack拼接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和拼接不同，堆叠不是将元素拆分重装，而是简单的将各参与堆叠的对象分装到一个更高维度的张量里。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.]]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([a, b])   # 堆叠之后，生成一个三维张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([a, b]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意对比二者区别，拼接之后维度不变，堆叠之后维度升高。拼接是把一个个元素单独提取出来之后再放到二维张量中，而堆叠则是直接将两个二维张量封装到一个三维张量中，**因此，堆叠的要求更高，参与堆叠的张量必须形状完全相同。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a, c])                # 横向拼接时，对行数没有一致性要求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [2, 3] at entry 0 and [3, 3] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10876/235654242.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m               \u001b[1;31m# 维度不匹配时也会报错\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [2, 3] at entry 0 and [3, 3] at entry 1"
     ]
    }
   ],
   "source": [
    "torch.stack([a, c])               # 维度不匹配时也会报错"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 8 张量的维度变换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此前我们介绍过，通过reshape方法，能够灵活调整张量的形状。而在实际操作张量进行计算时，往往需要另外进行降维和升维的操作，当我们需要除去不必要的维度时，可以使用squeeze函数，而需要手动升维时，则可采用unsqueeze函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 squeeze降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.],\n",
       "          [0.],\n",
       "          [0.]]]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.zeros(1, 1, 3, 1)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3, 1])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t张量解释：一个包含一个三维的四维张量，三维张量只包含一个三行一列的二维张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.squeeze(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.squeeze(t).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转化后生成了一个一维张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3, 2, 1, 2])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.zeros(1, 1, 3, 2, 1, 2)\n",
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.squeeze(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.squeeze(t1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单理解，squeeze就相当于提出了shape返回结果中的1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 unsqueeze升维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 1, 2])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.zeros(1, 2, 1, 2)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[0., 0.]],\n",
       "\n",
       "          [[0., 0.]]]]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(t, dim = 0) # 在第1个维度索引上升高1个维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2, 1, 2])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(t, dim = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 1, 1, 2])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(t, dim = 2).shape # 在第3个维度索引上升高1个维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 1, 2, 1])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(t, dim = 4).shape # 在第5个维度索引上升高1个维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意理解维度和shape返回结果一一对应的关系，shape返回的序列有几个元素，张量就有多少维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 9 张量的广播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.arange(3)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 4])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 + t1   # 对应位置元素依次相加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 标量和任意形状的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 + 1                                     # 1是标量，可以看成是零维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 二维加零维\n",
    "t1 + torch.tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.zeros((3, 4))\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 相同维度、不同形状的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 5])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = torch.zeros(3, 4, 5)\n",
    "t3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以将t3解释为：t3是3个二维张量组成了三维张量，并且这些每个二维张量，都是由四个包含五个元素的一维张量所组成。由二维拓展至三维，即可拓展至N维。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们以t2为例，来探讨相同维度、不同形状的张量之间的广播规则。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.1 2维张量的广播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t21 = torch.ones(1, 4)\n",
    "t21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t21.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t21的形状是（1， 4），和t2的形状（3， 4）在第一个分量上取值不同，但该分量上t21取值为1，因此可以广播，也就可以进行计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t21 + t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pics/张量/广播1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意理解：此处的广播相当于将t22的形状（1， 4）拓展成了t2的（3， 4），也就是复制了第一行三次，然后二者进行相加。当然，也可以理解成t22的第一行和t2的三行分别进行了相加。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t22 = torch.ones(3, 1)\n",
    "t22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t22.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t22 + t2              # 形状为（3，1）的张量和形状为（3，4）的张量相加，可以广播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t2和t22实际计算过程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pics/张量/广播2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t23 = torch.ones(2, 4)\n",
    "t23.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：此时t2和t23的形状第一个分量维度不同，但二者取值均不为1，因此无法广播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10876/1278058454.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mt2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mt23\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "t2 + t23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [2]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t24 = torch.arange(3).reshape(3, 1)\n",
    "t24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t25 = torch.arange(3).reshape(1, 3)\n",
    "t25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1]), torch.Size([1, 3]))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t24.shape, t25.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时，t24的形状是（3， 1），而t25的形状是（1， 3），二者的形状在两个份量上均不相同，但都有存在1的情况，因此也是可以广播的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [1, 2, 3],\n",
       "        [2, 3, 4]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t24 + t25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pics/张量/广播3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.2 3维张量的广播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = torch.zeros(3, 4, 5)\n",
    "t31 = torch.ones(3, 4, 1)\n",
    "t3 + t31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t32 = torch.ones(3, 1, 5)\n",
    "t32 + t3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两个张量的形状上有两个分量不同时，只要不同的分量仍然有一个取值为1，则仍然可以广播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 5])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t33 = torch.ones(1, 1, 5)\n",
    "t33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 + t33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t3和t33计算过程如下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pics/张量/广播4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：此处标注的两次广播，我们也可认为上述全部过程的实现是一次“大的”广播。同时，此处最开始的t33也就相当于一个一维的、包含五个元素的张量，因此上述过程也可视为一个一维张量和一个三维张量计算时的广播过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 不同维度的张量广播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在理解相同维度、不同形状的张量广播之后，对于不同维度的张量之间的广播其实就会容易很多，因为对于不同维度的张量，我们首先可以将低维的张量升维，然后依据相同维度不同形状的张量广播规则进行广播。而低维向量的升维也非常简单，只需将更高维度方向的形状填充为1即可，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 二维张量转化为三维张量\n",
    "t2 = torch.arange(4).reshape(2, 2)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1],\n",
       "         [2, 3]]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转化为三维张量\n",
    "t2.reshape(1, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转化之后表示只包含一个二维张量的三维张量，且二维张量就是t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0, 1],\n",
       "          [2, 3]]]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转化为四维张量\n",
    "t2.reshape(1, 1, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转化之后表示只包含一个三维张量的四维张量，且三维张量只包含一个二维张量，且二维张量就是t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = torch.zeros(3, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t3和t2的计算过程，就相当于形状为（1，2，2）和（3，2，2）的两个张量进行计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1.],\n",
       "         [2., 3.]],\n",
       "\n",
       "        [[0., 1.],\n",
       "         [2., 3.]],\n",
       "\n",
       "        [[0., 1.],\n",
       "         [2., 3.]]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 + t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1.],\n",
       "         [2., 3.]],\n",
       "\n",
       "        [[0., 1.],\n",
       "         [2., 3.]],\n",
       "\n",
       "        [[0., 1.],\n",
       "         [2., 3.]]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 + t2.reshape(1, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 10 张量计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 逐点计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pointwise Ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch中逐点运算大部分都是可以针对Tensor中每个元素都进行的数学科学运算，并且都是较为通用的数学科学运算，和NumPy中针对Array的科学运算类似。在PyTorch中文文档中有全部运算符的相关介绍，此处仅针对常用计算函数进行介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逐点运算主要包括数学基本运算、数值调整运算和数据科学运算三块，相关函数如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.1 基本数学计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**函数**|**描述**|\n",
    "| :------:| :------: |\n",
    "| torch.add(t1，t2 )      | t1、t2两个张量逐个元素相加，等效于t1+t2     |\n",
    "| torch.subtract(t1，t2) | t1、t2两个张量逐个元素相减，等效于t1-t2      |\n",
    "| torch.multiply(t1，t2) | t1、t2两个张量逐个元素相乘，等效于t1\\*t2            |\n",
    "| torch.divide(t1，t2)   | t1、t2两个张量逐个元素相除，等效于t1/t2            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.tensor([1, 2])\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.tensor([3, 4])\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 6])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 6])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 + t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3333, 0.5000])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 / t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.2 数值调整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**函数**|**描述**|\n",
    "| :------:| :------: |\n",
    "| torch.abs(t)        | 返回绝对值 | \n",
    "| torch.ceil(t)       | 向上取整 | \n",
    "| torch.floor(t)      | 向下取整 |\n",
    "| torch.round(t)      | 四舍五入取整 |\n",
    "| torch.neg(t)      | 返回相反的数 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0812, -0.6621, -0.6504,  0.3146,  0.6597])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(5)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0., -1., -1.,  0.,  1.])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0812, 0.6621, 0.6504, 0.3146, 0.6597])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0812,  0.6621,  0.6504, -0.3146, -0.6597])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.neg(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注**：虽然此类型函数是数值调整函数，但并不会对原对象进行调整，而是输出新的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5184, -0.4910, -0.1381, -0.2500, -0.4295])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t    # t本身并未发生变化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而若要对原对象本身进行修改，则可考虑使用`方法_()`的表达形式，对对象本身进行修改。此时方法就是上述同名函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0812, 0.6621, 0.6504, 0.3146, 0.6597])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.abs_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0812, 0.6621, 0.6504, 0.3146, 0.6597])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0812, -0.6621, -0.6504, -0.3146, -0.6597])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.neg_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0812, -0.6621, -0.6504, -0.3146, -0.6597])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了上述数值调整函数有对应的同名方法外，本节介绍的许多科学计算都有同名方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9220, 0.5158, 0.5218, 0.7300, 0.5170])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.exp_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9220, 0.5158, 0.5218, 0.7300, 0.5170])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.3 常用科学计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**数学运算函数**|**数学公式**|**描述**|\n",
    "| :------:| :------: | :------: |\n",
    "| 幂运算 |\n",
    "| torch.exp(t)        |$ y_{i} = e^{x_{i}} $ | 返回以e为底、t中元素为幂的张量 | \n",
    "| torch.expm1(t)         | $ y_{i} = e^{x_{i}} $ - 1 |对张量中的所有元素计算exp（x） - 1|\n",
    "| torch.exp2(t)          | $ y_{i} = 2^{x_{i}} $ |逐个元素计算2的t次方。 | \n",
    "| torch.pow(t,n)       | $\\text{out}_i = x_i ^ \\text{exponent} $ | 返回t的n次幂 | \n",
    "| torch.sqrt(t)       |$ \\text{out}_{i} = \\sqrt{\\text{input}_{i}} $ | 返回t的平方根 | \n",
    "| torch.square(t)        |$ \\text{out}_i = x_i ^ \\text{2} $ | 返回输入的元素平方。                     | \n",
    "| 对数运算 |\n",
    "| torch.log10(t)      |$ y_{i} = \\log_{10} (x_{i}) $ | 返回以10为底的t的对数 | \n",
    "| torch.log(t)  |$ y_{i} = \\log_{e} (x_{i}) $| 返回以e为底的t的对数 |\n",
    "| torch.log2(t)          |$ y_{i} = \\log_{2} (x_{i}) $| 返回以2为底的t的对数                         | \n",
    "| torch.log1p(t)         |$ y_i = \\log_{e} (x_i $ + 1)| 返回一个加自然对数的输入数组。     | \n",
    "| 三角函数运算|\n",
    "| torch.sin(t)           |三角正弦。                               | \n",
    "| torch.cos(t)           | 元素余弦。                               | \n",
    "| torch.tan(t)           |逐元素计算切线。                         | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor的大多数科学计算只能作用于tensor对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pow() received an invalid combination of arguments - got (int, int), but expected one of:\n * (Tensor input, Tensor exponent, *, Tensor out)\n * (Number self, Tensor exponent, *, Tensor out)\n * (Tensor input, Number exponent, *, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10876/1681691539.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 计算2的2次方\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: pow() received an invalid combination of arguments - got (int, int), but expected one of:\n * (Tensor input, Tensor exponent, *, Tensor out)\n * (Number self, Tensor exponent, *, Tensor out)\n * (Tensor input, Number exponent, *, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "# 计算2的2次方\n",
    "torch.pow(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow(torch.tensor(2), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理解：相比于Python原生数据类型，张量是一类更加特殊的对象，例如张量可以指定运行在CPU或者GPU上，因此很多张量的科学计算函数都不允许张量和Python原生的数值型对象混合使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor的大多数科学运算具有一定的静态性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(1, 4)\n",
    "t.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.7183,  7.3891, 20.0855])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(t) # 有的会报错，这里我没有：exp_vml_cpu not implemented for 'Long'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的是，虽然Python是动态编译的编程语言，但在PyTorch中，由于会涉及GPU计算，因此很多时候元素类型不会在实际执行函数计算时进行调整。此处的科学运算大多数都要求对象类型是浮点型，我们需要提前进行类型转化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = t.float()\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.7183,  7.3891, 20.0855])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.7183,  6.3891, 19.0855])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.expm1(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注，此处返回结果是$e^{t} - 1$，在数值科学计算中，expm1函数和log1p函数是一对对应的函数关系，后面再介绍log1p的时候会讲解这对函数的实际作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 8.])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp2(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 9])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow(t, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 9])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.square(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.4142, 1.7321])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.4142, 1.7321])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow(t1, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开根号也就相当于0.5次幂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.3010, 0.4771])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log10(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.6931, 1.0986])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 1.0000, 1.5850])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log2(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时，我们也可简单回顾幂运算和对数运算之间的关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(torch.log(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp2(torch.log2(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(-1).float()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp2(torch.log2(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 排序运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在PyTorch中，sort排序函数将同时返回排序结果和对应的索引值的排列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.1 一维张量的排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 3., 2.])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([1.0, 3.0, 2.0])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([1., 2., 3.]),\n",
       "indices=tensor([0, 2, 1]))"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 升序排列\n",
    "torch.sort(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([3., 2., 1.]),\n",
       "indices=tensor([1, 2, 0]))"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 降序排列\n",
    "torch.sort(t, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.sort(t)\n",
    "a.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.2 二维张量的排序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和上述过程类似，在进行排序过程中，二维张量也可以按行或者按列进行排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7416, -1.1379, -1.0080, -0.3674],\n",
       "        [ 0.5243,  0.2535,  1.7975, -0.7141],\n",
       "        [ 2.0279, -0.1804, -0.5531, -2.0880]])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t22 = torch.randn(3, 4)             # 创建二维随机数张量\n",
    "t22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([[-1.1379, -1.0080, -0.7416, -0.3674],\n",
       "        [-0.7141,  0.2535,  0.5243,  1.7975],\n",
       "        [-2.0880, -0.5531, -0.1804,  2.0279]]),\n",
       "indices=tensor([[1, 2, 0, 3],\n",
       "        [3, 1, 0, 2],\n",
       "        [3, 2, 1, 0]]))"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 默认情况下，是按照行进行升序排序\n",
    "torch.sort(t22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([[ 0.0524,  0.0274, -0.0721, -1.1111],\n",
       "        [ 0.4069, -0.0838, -0.1148, -1.5132],\n",
       "        [-0.4375, -0.6726, -0.9050, -0.9713]]),\n",
       "indices=tensor([[3, 0, 2, 1],\n",
       "        [3, 2, 0, 1],\n",
       "        [2, 3, 1, 0]]))"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 修改dim和descending参数，使得按列进行降序排序\n",
    "torch.sort(t22, dim = 1, descending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 规约运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "规约运算指的是针对某张量进行某种总结，最后得出一个具体总结值的函数。此类函数主要包含了数据科学领域内的诸多统计分析函数，如均值、极值、方差、中位数函数等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**函数**|**描述**|\n",
    "| :------:| :------: |\n",
    "| torch.mean(t)        | 返回张量均值 | \n",
    "| torch.var(t)       | 返回张量方差 | \n",
    "| torch.std(t)        | 返回张量标准差 | \n",
    "| torch.var_mean(t)       | 返回张量方差和均值 | \n",
    "| torch.std_mean(t)       | 返回张量标准差和均值 | \n",
    "| torch.max(t)        | 返回张量最大值 | \n",
    "| torch.argmax(t)        | 返回张量最大值索引 | \n",
    "| torch.min(t)       | 返回张量最小值 | \n",
    "| torch.argmin(t)       | 返回张量最小值索引 | \n",
    "| torch.median(t)        | 返回张量中位数 | \n",
    "| torch.sum(t)       | 返回张量求和结果 | \n",
    "| torch.logsumexp(t)       | 返回张量各元素求和结果，适用于数据量较小的情况 | \n",
    "| torch.prod(t)        | 返回张量累乘结果 | \n",
    "| torch.dist(t1, t2)        | 计算两个张量的闵式距离，可使用不同范式 |\n",
    "| torch.topk(t)        | 返回t中最大的k个值对应的指标 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.1 一般统计分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成浮点型张量\n",
    "t = torch.arange(10).float()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.5000)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算均值\n",
    "torch.mean(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.0277), tensor(4.5000))"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算标准差、均值\n",
    "torch.std_mean(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算最大值\n",
    "torch.max(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回最大值的索引\n",
    "torch.argmax(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算中位数\n",
    "torch.median(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(45.)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 求和\n",
    "torch.sum(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 求积\n",
    "torch.prod(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.prod(torch.tensor([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.tensor([1.0, 2])\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 4.])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.tensor([3.0, 4])\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.2 距离计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dist函数可计算闵式距离（闵可夫斯基距离），通过输入不同的p值，可以计算多种类型的距离，如欧式距离、街道距离等。闵可夫斯基距离公式如下：      \n",
    "<center> $ D(x,y) = (\\sum^{n}_{u=1}|x_u-y_u|^{p})^{1/p}$ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p取值为2时，计算欧式距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8284)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(t1, t2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8284)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(torch.tensor(8.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p取值为1时，计算街道距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(t1, t2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.3 规约计算的维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于规约运算是一个序列返回一个结果，因此若是针对高维张量，则可指定某维度进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个3*3的二维张量\n",
    "t2 = torch.arange(12).float().reshape(3, 4)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12., 15., 18., 21.])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 按照第一个维度求和（每次计算三个）、按列求和\n",
    "torch.sum(t2, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6., 22., 38.])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 按照第二个维度求和（每次计算四个）、按行求和\n",
    "torch.sum(t2, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]],\n",
       "\n",
       "        [[12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.],\n",
       "         [20., 21., 22., 23.]]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个2*3*4的三维张量\n",
    "t3 = torch.arange(24).float().reshape(2, 3, 4)\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 14., 16., 18.],\n",
       "        [20., 22., 24., 26.],\n",
       "        [28., 30., 32., 34.]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(t3, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 15., 18., 21.],\n",
       "        [48., 51., 54., 57.]])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(t3, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6., 22., 38.],\n",
       "        [54., 70., 86.]])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(t3, dim = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dim参数和shape返回结果一一对应"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 比较运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较运算是一类较为简单的运算类型，和Python原生的布尔运算类似，常用于不同张量之间的逻辑运算，最终返回逻辑运算结果（逻辑类型张量）。基本比较运算函数如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center>Tensor比较运算函数</center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**函数**|**描述**|\n",
    "| :------:| :------: |\n",
    "| torch.eq(t1, t2)        | 比较t1、t2各元素是否相等，等效==| \n",
    "| torch.equal(t1, t2)       | 判断两个张量是否是相同的张量 | \n",
    "| torch.gt(t1, t2)        | 比较t1各元素是否大于t2各元素，等效>| \n",
    "| torch.lt(t1, t2)        | 比较t1各元素是否小于t2各元素，等效<| \n",
    "| torch.ge(t1, t2)        | 比较t1各元素是否大于或等于t2各元素，等效>=| \n",
    "| torch.le(t1, t2)        | 比较t1各元素是否小于等于t2各元素，等效<=| \n",
    "| torch.ne(t1, t2)        | 比较t1、t2各元素是否不相同，等效!=| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([1.0, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.tensor([1.0, 2, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False, False])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 == t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(t1, t2) # 判断t1、t2是否是相同的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False, False])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True, False])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 > t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True, False])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 >= t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# 11 张量的线性代数计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch中并未设置单独的矩阵对象类型，因此PyTorch中，二维张量就相当于矩阵对象，并且拥有一系列线性代数相关函数和方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实际机器学习和深度学习建模过程中，矩阵或者高维张量都是基本对象类型，而矩阵所涉及到的线性代数理论也是深度学习用户必备的基本数学基础。因此，本节在介绍张量的线性代数运算时，也会回顾基本的矩阵运算，及其基本线性代数的数学理论基础，以期在强化张量的线性代数运算过程中，也进一步夯实同学的线性代数数学基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外，在实际的深度学习建模过程中，往往会涉及矩阵的集合，也就是三维甚至是四维张量的计算，因此在部分场景中，我们也将把二维张量计算拓展到更高维的张量计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## 11.1 BLAS和LAPACK概览"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLAS（Basic Linear Algeria Subprograms）和LAPACK（Linear Algeria Package）模块提供了完整的线性代数基本方法，由于涉及到函数种类较多，因此此处对其进行简单分类，具体包括：\n",
    "- 矩阵的形变及特殊矩阵的构造方法：包括矩阵的转置、对角矩阵的创建、单位矩阵的创建、上/下三角矩阵的创建等；\n",
    "- 矩阵的基本运算：包括矩阵乘法、向量内积、矩阵和向量的乘法等，当然，此处还包含了高维张量的基本运算，将着重探讨矩阵的基本运算拓展至三维张量中的基本方法；\n",
    "- 矩阵的线性代数运算：包括矩阵的迹、矩阵的秩、逆矩阵的求解、伴随矩阵和广义逆矩阵等；\n",
    "- 矩阵分解运算：特征分解、奇异值分解和SVD分解等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相关内容如果涉及数学基础，将在讲解过程中逐步补充。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 矩阵的形变及特殊矩阵构造方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵的形变方法其实也就是二维张量的形变方法，在此基础上本节将补充转置的基本方法。另外，在实际线性代数运算过程中，经常涉及一些特殊矩阵，如单位矩阵、对角矩阵等，相关创建方法如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**函数**|**描述**|\n",
    "| :------:| :------: |\n",
    "| torch.t(t)        | t转置| \n",
    "| torch.eye(n)       | 创建包含n个分量的单位矩阵 | \n",
    "| torch.diag(t1)        | 以t1中各元素，创建对角矩阵 | \n",
    "| torch.triu(t)        | 取矩阵t中的上三角矩阵 | \n",
    "| torch.tril(t)        | 取矩阵t中的下三角矩阵 | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个2*3的矩阵\n",
    "t1 = torch.arange(1, 7).reshape(2, 3).float()\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4.],\n",
       "        [2., 5.],\n",
       "        [3., 6.]])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转置\n",
    "torch.t(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4.],\n",
       "        [2., 5.],\n",
       "        [3., 6.]])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 矩阵的转置就是每个元素行列位置互换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(5)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0],\n",
       "        [0, 0, 2, 0, 0],\n",
       "        [0, 0, 0, 3, 0],\n",
       "        [0, 0, 0, 0, 4]])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 2, 0, 0],\n",
       "        [0, 0, 0, 0, 3, 0],\n",
       "        [0, 0, 0, 0, 0, 4],\n",
       "        [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对角线向上偏移一位\n",
    "torch.diag(t, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 2, 0, 0, 0],\n",
       "        [0, 0, 0, 3, 0, 0],\n",
       "        [0, 0, 0, 0, 4, 0]])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对角线向下偏移一位\n",
    "torch.diag(t, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.arange(9).reshape(3, 3)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [0, 4, 5],\n",
       "        [0, 0, 8]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取上三角矩阵\n",
    "torch.triu(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [0, 7, 8]])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 上三角矩阵向左下偏移一位\n",
    "torch.triu(t1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [0, 0, 5],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 上三角矩阵向右上偏移一位\n",
    "torch.triu(t1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [3, 4, 0],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下三角矩阵\n",
    "torch.tril(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 矩阵的基本运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵不同于普通的二维数组，其具备一定的线性代数含义，而这些特殊的性质，其实就主要体现在矩阵的基本运算上。课程中常见的矩阵基本运算如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center>矩阵的基本运算</center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**函数**|**描述**|\n",
    "| :------:| :------: |\n",
    "| torch.dot(t1, t2)        | 计算t1、t2张量内积 | \n",
    "| torch.mm(t1, t2)        | 矩阵乘法 | \n",
    "| torch.mv(t1, t2)        | 矩阵乘向量 | \n",
    "| torch.bmm(t1, t2)        | 批量矩阵乘法 | \n",
    "| torch.addmm(t, t1, t2)        | 矩阵相乘后相加 | \n",
    "| torch.addbmm(t, t1, t2)        | 批量矩阵相乘后相加 | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dot\\vdot：点积计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，在PyTorch中，dot和vdot只能作用于一维张量，且对于数值型对象，二者计算结果并没有区别，两种函数只在进行复数运算时会有区别。更多复数运算的规则，我们将在涉及复数运算的场景中再进行详细说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(1, 4)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(t, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.vdot(t, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "1D tensors expected, but got 2D and 2D tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10876/2514658519.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 不能进行除了一维张量以外的计算\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: 1D tensors expected, but got 2D and 2D tensors"
     ]
    }
   ],
   "source": [
    "# 不能进行除了一维张量以外的计算\n",
    "torch.dot(t1, t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mm：矩阵乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再PyTorch中，矩阵乘法其实是一个函数簇，除了矩阵乘法以外，还有批量矩阵乘法、矩阵相乘相加、批量矩阵相乘相加等等函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.arange(1, 7).reshape(2, 3)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.arange(1, 10).reshape(3, 3)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  4,  9],\n",
       "        [16, 25, 36]])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对应位置元素相乘\n",
    "t1 * t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30, 36, 42],\n",
       "        [66, 81, 96]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩阵乘法\n",
    "torch.mm(t1, t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵乘法执行过程如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pics/张量/矩阵乘法.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mv：矩阵和向量相乘      \n",
    "\n",
    "PyTorch中提供了一类非常特殊的矩阵和向量相乘的函数，矩阵和向量相乘的过程我们可以看成是先将向量转化为列向量然后再相乘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "met = torch.arange(1, 7).reshape(2, 3)\n",
    "met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = torch.arange(1, 4)\n",
    "vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实际执行向量和矩阵相乘的过程中，需要矩阵的列数和向量的元素个数相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 32])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mv(met, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3]])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.reshape(3, 1)             # 转化为列向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14],\n",
       "        [32]])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(met, vec.reshape(3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 32])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(met, vec.reshape(3, 1)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**理解**：mv函数本质上提供了一种二维张量和一维张量相乘的方法，在线性代数运算过程中，有很多矩阵乘向量的场景，典型的如线性回归的求解过程，通常情况下我们需要将向量转化为列向量（或者某些编程语言就默认向量都是列向量）然后进行计算，但PyTorch中单独设置了一个矩阵和向量相乘的方法，从而简化了行/列向量的理解过程和将向量转化为列向量的转化过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- bmm：批量矩阵相乘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所谓批量矩阵相乘，指的是三维张量的矩阵乘法。根据此前对张量结构的理解，我们知道，三维张量就是一个包含了多个相同形状的矩阵的集合。例如，一个（3， 2， 2）的张量，本质上就是一个包含了3个2*2矩阵的张量。而三维张量的矩阵相乘，则是三维张量内部各对应位置的矩阵相乘。由于张量的运算往往涉及二维及以上，因此批量矩阵相乘也有非常多的应用场景。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2],\n",
       "         [ 3,  4]],\n",
       "\n",
       "        [[ 5,  6],\n",
       "         [ 7,  8]],\n",
       "\n",
       "        [[ 9, 10],\n",
       "         [11, 12]]])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = torch.arange(1, 13).reshape(3, 2, 2)\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3],\n",
       "         [ 4,  5,  6]],\n",
       "\n",
       "        [[ 7,  8,  9],\n",
       "         [10, 11, 12]],\n",
       "\n",
       "        [[13, 14, 15],\n",
       "         [16, 17, 18]]])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4 = torch.arange(1, 19).reshape(3, 2, 3)\n",
    "t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2, 2]), torch.Size([3, 2, 3]))"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3.shape, t4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  9,  12,  15],\n",
       "         [ 19,  26,  33]],\n",
       "\n",
       "        [[ 95, 106, 117],\n",
       "         [129, 144, 159]],\n",
       "\n",
       "        [[277, 296, 315],\n",
       "         [335, 358, 381]]])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(t3, t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Point:**     \n",
    "- 三维张量包含的矩阵个数需要相同；\n",
    "- 每个内部矩阵，需要满足矩阵乘法的条件，也就是左乘矩阵的行数要等于右乘矩阵的列数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "addmm：矩阵相乘后相加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "addmm函数结构：addmm(input, mat1, mat2, beta=1, alpha=1)       \n",
    "输出结果：beta * input + alpha * (mat1 * mat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(3)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30, 36, 42],\n",
       "        [66, 81, 96]])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(t1, t2)                    # 矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30, 37, 44],\n",
       "        [66, 82, 98]])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.addmm(t, t1, t2)              # 先乘法后相加   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[300, 360, 420],\n",
       "        [660, 810, 960]])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.addmm(t, t1, t2, beta = 0, alpha = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "addbmm：批量矩阵相乘后相加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和addmm类似，都是先乘后加，并且可以设置权重。不同的是addbmm是批量矩阵相乘，并且，在相加的过程中也是矩阵相加，而非向量加矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(6).reshape(2, 3)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2],\n",
       "         [ 3,  4]],\n",
       "\n",
       "        [[ 5,  6],\n",
       "         [ 7,  8]],\n",
       "\n",
       "        [[ 9, 10],\n",
       "         [11, 12]]])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3],\n",
       "         [ 4,  5,  6]],\n",
       "\n",
       "        [[ 7,  8,  9],\n",
       "         [10, 11, 12]],\n",
       "\n",
       "        [[13, 14, 15],\n",
       "         [16, 17, 18]]])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  9,  12,  15],\n",
       "         [ 19,  26,  33]],\n",
       "\n",
       "        [[ 95, 106, 117],\n",
       "         [129, 144, 159]],\n",
       "\n",
       "        [[277, 296, 315],\n",
       "         [335, 358, 381]]])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(t3, t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[381, 415, 449],\n",
       "        [486, 532, 578]])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.addbmm(t, t3, t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注：**addbmm会在原来三维张量基础之上，对其内部矩阵进行求和"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4 矩阵的线性代数运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果说矩阵的基本运算是矩阵基本性质，那么矩阵的线性代数运算，则是我们利用矩阵数据类型在求解实际问题过程中经常涉及到的线性代数方法，具体相关函数如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center>矩阵的线性代数运算</center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**函数**|**描述**|\n",
    "| :------:| :------: |\n",
    "| torch.trace(A)       | 矩阵的迹 |\n",
    "| matrix_rank(A)       | 矩阵的秩 |\n",
    "| torch.det(A)         | 计算矩阵A的行列式 |  \n",
    "| torch.inverse(A)        | 矩阵求逆 | \n",
    "| torch.lstsq(A,B)        | 最小二乘法 | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时，由于线性代数所涉及的数学基础知识较多，从实际应用的角度出发，我们将有所侧重的介绍实际应用过程中需要掌握的相关内容，并通过本节末尾的实际案例，来加深线性代数相关内容的理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.1 矩阵的迹（trace）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;矩阵的迹的运算相对简单，就是矩阵对角线元素之和，在PyTorch中，可以使用trace函数进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [4., 5.]])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2], [4, 5]]).float()  \n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.trace(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，对于矩阵的迹来说，计算过程不需要是方阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.arange(1, 7).reshape(2, 3)\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.trace(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.2 矩阵的秩(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵的秩（rank），是指矩阵中行或列的极大线性无关数，且矩阵中行、列极大无关数总是相同的，任何矩阵的秩都是唯一值，满秩指的是方阵（行数和列数相同的矩阵）中行数、列数和秩相同，满秩矩阵有线性唯一解等重要特性，而其他矩阵也能通过求解秩来降维，同时，秩也是奇异值分解等运算中涉及到的重要概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- matrix_rank计算矩阵的秩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(1, 5).reshape(2, 2).float()\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matrix_rank(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [2., 4.]])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([[1, 2], [2, 4]]).float()\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于矩阵B来说，第一列和第二列明显线性相关，最大线性无关组只有1组，因此矩阵的秩计算结果为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matrix_rank(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.3 矩阵的行列式(det)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;所谓行列式，我们可以简单将其理解为矩阵的一个基本性质或者属性，通过行列式的计算，我们能够知道矩阵是否可逆，从而可以进一步求解矩阵所对应的线性方程。当然，更加专业的解释，行列式的作为一个基本数学工具，实际上就是矩阵进行线性变换的伸缩因子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于任何一个n维方正，行列式计算过程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pics/张量/行列式公式.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更为简单的情况，如果对于一个2*2的矩阵，行列式的计算就是主对角线元素之积减去另外两个元素之积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [4., 5.]])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2], [4, 5]]).float()     # 秩的计算要求浮点型张量\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.det(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [2., 4.]])"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.det(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A的行列式计算过程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pics/张量/行列式计算.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于行列式的计算，要求二维张量必须是方正，也就是行列数必须一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.arange(1, 7).reshape(2, 3)\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "A must be batches of square matrices, but they are 3 by 2 matrices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10876/3041031823.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: A must be batches of square matrices, but they are 3 by 2 matrices"
     ]
    }
   ],
   "source": [
    "torch.det(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.4 线性方程组的矩阵表达形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在正式进入到更进一步矩阵运算的讨论之前，我们需要对矩阵建立一个更加形象化的理解。通常来说，我们会把高维空间中的一个个数看成是向量，而由这些向量组成的数组看成是一个矩阵。例如：（1，2），（3，4）是二维空间中的两个点，矩阵A就代表这两个点所组成的矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(1, 5).reshape(2, 2).float()\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15df3105ec8>]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAV7klEQVR4nO3dcaxc5X3m8e9TcwsORNjENym1DWZblE2AgLMjJ61RG0gBJ02AZiOt05TSishSSnahG7EKrEQU+kfoIiXZrTZLrIBKsiTgBcN6aQhYi6uUspiMjYmxDa0LNHCL5BuMATeWGzvP/jGv2/Fl7r1n7PFc++X5SCOfec975vzm8PLcueecua9sExER9fqFmS4gIiKOrAR9RETlEvQREZVL0EdEVC5BHxFRueNmuoBe5s2b50WLFs10GRERx4wNGzb8xPZor3VHZdAvWrSIdrs902VERBwzJP39ZOty6iYionIJ+oiIyiXoIyIql6CPiKhcgj4ionKNg17SLElPSnqgx7rjJd0tabuk9ZIWda27vrQ/K+mSAdUdEVGN+58cY+nNj3DGF/6CpTc/wv1Pjg309fv5RH8NsG2SdVcBr9r+VeCrwJ8CSHovsBw4C1gGfF3SrEMvNyKiLvc/Ocb1qzcztmsPBsZ27eH61ZsHGvaNgl7SAuC3gW9O0uUy4I6yfA/wYUkq7XfZ3mv7eWA7sOTwSo6IqMctDz3Lnp/tP6htz8/2c8tDzw5sH00/0X8N+E/AzydZPx94EcD2PuA14B3d7cVLpe1NJK2Q1JbUHh8fb1hWRMSx7R927emr/VBMG/SSPgbssL1hYHvtwfZK2y3brdHRnt/ijYiozi/Pmd1X+6Fo8ol+KXCppBeAu4ALJf3PCX3GgIUAko4DTgZe6W4vFpS2iIgArrvk3cweOfjS5eyRWVx3ybsHto9pg9729bYX2F5E58LqI7Z/b0K3NcCVZfmTpY9L+/JyV84ZwJnAEwOrPiLiGHf54vl8+RPnMH/ObATMnzObL3/iHC5f3PMs9yE55D9qJukmoG17DXAb8G1J24GddH4gYHuLpFXAVmAfcLXt/ZO9ZkTEW9Hli+cPNNgn0tE4OXir1XL+emVERHOSNthu9VqXb8ZGRFQuQR8RUbkEfURE5RL0ERGVS9BHRFQuQR8RUbkEfURE5RL0ERGVS9BHRFQuQR8RUbkEfURE5RL0ERGVS9BHRFQuQR8RUbkEfURE5RL0ERGVm3aGKUknAD8Aji/977H9xQl9vgpcUJ6+DXin7Tll3X5gc1n3Y9uXDqb0iIhooslUgnuBC23vljQCPCrpQduPH+hg+48PLEv698Diru332D5vUAVHRER/mkwObtu7y9OR8phq/sFPAd8dQG0RETEAjc7RS5olaROwA1hre/0k/U4HzgAe6Wo+QVJb0uOSLp9iHytKv/b4+HjjNxAREVNrFPS295fTLwuAJZLOnqTrcjrn8Pd3tZ1eJqz9XeBrkn5lkn2stN2y3RodHW3+DiIiYkp93XVjexewDlg2SZflTDhtY3us/Psc8JccfP4+IiKOsGmDXtKopDlleTZwEfBMj37/GpgL/L+utrmSji/L84ClwNaBVB4REY00uevmVOAOSbPo/GBYZfsBSTcBbdtrSr/lwF22uy/Uvgf4hqSfl21vtp2gj4gYIh2cy0eHVqvldrs902VERBwzJG0o10PfJN+MjYioXII+IqJyCfqIiMol6CMiKpegj4ioXII+IqJyCfqIiMol6CMiKpegj4ioXII+IqJyCfqIiMol6CMiKpegj4ioXII+IqJyCfqIiMol6CMiKtdkKsETJD0h6SlJWyR9qUefP5A0LmlTeXyma92Vkv62PK4c9BuIiIipNZlKcC9woe3dkkaARyU9aPvxCf3utv257gZJpwBfBFqAgQ2S1th+dRDFR0TE9Kb9RO+O3eXpSHk0nX/wEmCt7Z0l3NcCyw6p0oiIOCSNztFLmiVpE7CDTnCv79Ht30r6kaR7JC0sbfOBF7v6vFTaeu1jhaS2pPb4+HjzdxAREVNqFPS299s+D1gALJF09oQu/wdYZPt9dD6139FvIbZX2m7Zbo2Ojva7eURETKKvu25s7wLWMeH0i+1XbO8tT78J/JuyPAYs7Oq6oLRFRMSQNLnrZlTSnLI8G7gIeGZCn1O7nl4KbCvLDwEXS5oraS5wcWmLiIghaXLXzanAHZJm0fnBsMr2A5JuAtq21wD/QdKlwD5gJ/AHALZ3SvoT4IfltW6yvXPQbyIiIiYnu+kNNMPTarXcbrdnuoyIiGOGpA22W73W5ZuxERGVS9BHRFQuQR8RUbkEfURE5RL0ERGVS9BHRFQuQR8RUbkEfURE5RL0ERGVS9BHRFQuQR8RUbkEfURE5RL0ERGVS9BHRFQuQR8RUbkmM0ydIOkJSU9J2iLpSz36/EdJW8vk4P9X0uld6/ZL2lQeawb9BiIiYmpNZpjaC1xoe7ekEeBRSQ/afryrz5NAy/ZPJX0W+C/Avyvr9pSJxSMiYgZM+4neHbvL05Hy8IQ+62z/tDx9nM4k4BERcRRodI5e0ixJm4AdwFrb66fofhXwYNfzEyS1JT0u6fIp9rGi9GuPj483KSsiIhpoFPS295fTLwuAJZLO7tVP0u8BLeCWrubTyzyGvwt8TdKvTLKPlbZbtlujo6P9vIeIiJhCX3fd2N4FrAOWTVwn6beA/wxcantv1zZj5d/ngL8EFh96uRER0a8md92MSppTlmcDFwHPTOizGPgGnZDf0dU+V9LxZXkesBTYOrDqIyJiWk3uujkVuEPSLDo/GFbZfkDSTUDb9ho6p2pOAv6XJIAf274UeA/wDUk/L9vebDtBHxExRNMGve0f0eN0i+0bu5Z/a5JtHwPOOZwCIyLi8OSbsRERlUvQR0RULkEfEVG5BH1EROUS9BERlUvQR0RULkEfEVG5BH1EROUS9BERlUvQR0RULkEfEVG5BH1EROUS9BERlUvQR0RULkEfEVG5BH1EROWaTCV4gqQnJD0laYukL/Xoc7ykuyVtl7Re0qKuddeX9mclXTLg+iMiYhpNPtHvBS60fS5wHrBM0gcn9LkKeNX2rwJfBf4UQNJ7geXAWXQmFP96mZIwIiKGZNqgd8fu8nSkPDyh22XAHWX5HuDD6kweexlwl+29tp8HtgNLBlJ5REQ00ugcvaRZkjYBO4C1ttdP6DIfeBHA9j7gNeAd3e3FS6Wt1z5WSGpLao+Pj/f1JiIiYnKNgt72ftvnAQuAJZLOHnQhtlfabtlujY6ODvrlIyLesvq668b2LmAdnfPt3caAhQCSjgNOBl7pbi8WlLaIiBiSJnfdjEqaU5ZnAxcBz0zotga4six/EnjEtkv78nJXzhnAmcATA6o9IiIaOK5Bn1OBO8rdMr8ArLL9gKSbgLbtNcBtwLclbQd20rnTBttbJK0CtgL7gKtt7z8SbyQiInpT54P30aXVarndbs90GRERxwxJG2y3eq3LN2MjIiqXoI+IqFyCPiKicgn6iIjKJegjIiqXoI+IqFyCPiKicgn6iIjKJegjIiqXoI+IqFyCPiKicgn6iIjKJegjIiqXoI+IqFyCPiKicgn6iIjKTTvDlKSFwLeAdwEGVtr+rxP6XAd8uus13wOM2t4p6QXgDWA/sG+yP4wfERFHRpOpBPcBn7e9UdLbgQ2S1treeqCD7VuAWwAkfRz4Y9s7u17jAts/GWThERHRzLSnbmy/bHtjWX4D2AbMn2KTTwHfHUx5ERFxuPo6Ry9pEbAYWD/J+rcBy4B7u5oNPCxpg6QVU7z2CkltSe3x8fF+yoqIiCk0DnpJJ9EJ8Gttvz5Jt48Dfz3htM35tt8PfAS4WtJv9NrQ9krbLdut0dHRpmVFRMQ0GgW9pBE6IX+n7dVTdF3OhNM2tsfKvzuA+4Alh1ZqREQcimmDXpKA24Bttr8yRb+Tgd8E/ndX24nlAi6STgQuBp4+3KIjIqK5JnfdLAWuADZL2lTabgBOA7B9a2n7HeBh2//Yte27gPs6Pys4DviO7e8PoO6IiGho2qC3/SigBv3+HPjzCW3PAeceYm0RETEA+WZsRETlEvQREZVL0EdEVC5BHxFRuQR9RETlEvQREZVL0EdEVC5BHxFRuQR9RETlEvQREZVL0EdEVC5BHxFRuQR9RETlEvQREZVL0EdEVK7JDFMLJa2TtFXSFknX9OjzIUmvSdpUHjd2rVsm6VlJ2yV9YdBvICIiptZkhql9wOdtbyzTAm6QtNb21gn9/sr2x7obJM0C/jtwEfAS8ENJa3psGxERR8i0n+htv2x7Y1l+A9gGzG/4+kuA7bafs/1PwF3AZYdabERE9K+vc/SSFgGLgfU9Vv+apKckPSjprNI2H3ixq89LTPJDQtIKSW1J7fHx8X7KioiIKTQOekknAfcC19p+fcLqjcDpts8F/gy4v99CbK+03bLdGh0d7XfziIiYRKOglzRCJ+TvtL164nrbr9veXZa/B4xImgeMAQu7ui4obRERMSRN7roRcBuwzfZXJunzS6UfkpaU130F+CFwpqQzJP0isBxYM6jiIyJiek3uulkKXAFslrSptN0AnAZg+1bgk8BnJe0D9gDLbRvYJ+lzwEPALOB221sG+xYiImIq6uTx0aXVarndbs90GRERxwxJG2y3eq3LN2MjIiqXoI+IqFyCPiKicgn6iIjKJegjIiqXoI+IqFyCPiKicgn6iIjKJegjIiqXoI+IqFyCPiKicgn6iIjKJegjIiqXoI+IqFyCPiKicgn6iIjKNZlKcKGkdZK2Stoi6ZoefT4t6UeSNkt6TNK5XeteKO2bJGU2kYiIIWsyleA+4PO2N0p6O7BB0lrbW7v6PA/8pu1XJX0EWAl8oGv9BbZ/MriyIyKiqWmD3vbLwMtl+Q1J24D5wNauPo91bfI4sGDAdUZExCHq6xy9pEXAYmD9FN2uAh7sem7gYUkbJK2Y4rVXSGpLao+Pj/dTVkRETKHJqRsAJJ0E3Atca/v1SfpcQCfoz+9qPt/2mKR3AmslPWP7BxO3tb2SzikfWq3W0TdjeUTEMarRJ3pJI3RC/k7bqyfp8z7gm8Bltl850G57rPy7A7gPWHK4RUdERHNN7roRcBuwzfZXJulzGrAauML233S1n1gu4CLpROBi4OlBFB4REc00OXWzFLgC2CxpU2m7ATgNwPatwI3AO4Cvd34usM92C3gXcF9pOw74ju3vD/INRETE1JrcdfMooGn6fAb4TI/254Bz37xFREQMS74ZGxFRuQR9RETlEvQREZVL0EdEVC5BHxFRuQR9RETlEvQREZVL0EdEVC5BHxFRuQR9RETlEvQREZVL0EdEVC5BHxFRuQR9RETlEvQREZVL0EdEVK7JVIILJa2TtFXSFknX9OgjSf9N0nZJP5L0/q51V0r62/K4ctBv4ID7nxxj6c2PcMYX/oKlNz/C/U+OHaldRUQcU5pMJbgP+LztjWX+1w2S1tre2tXnI8CZ5fEB4H8AH5B0CvBFoAW4bLvG9quDfBP3PznG9as3s+dn+wEY27WH61dvBuDyxfMHuauIiGPOtJ/obb9se2NZfgPYBkxMz8uAb7njcWCOpFOBS4C1tneWcF8LLBvoOwBueejZfw75A/b8bD+3PPTsoHcVEXHM6escvaRFwGJg/YRV84EXu56/VNoma+/12isktSW1x8fH+ymLf9i1p6/2iIi3ksZBL+kk4F7gWtuvD7oQ2yttt2y3RkdH+9r2l+fM7qs9IuKtpFHQSxqhE/J32l7do8sYsLDr+YLSNln7QF13ybuZPTLroLbZI7O47pJ3D3pXERHHnCZ33Qi4Ddhm+yuTdFsD/H65++aDwGu2XwYeAi6WNFfSXODi0jZQly+ez5c/cQ7z58xGwPw5s/nyJ87JhdiICJrddbMUuALYLGlTabsBOA3A9q3A94CPAtuBnwJ/WNbtlPQnwA/LdjfZ3jmw6rtcvnh+gj0ioodpg972o4Cm6WPg6knW3Q7cfkjVRUTEYcs3YyMiKpegj4ioXII+IqJyCfqIiMqpcx316CJpHPj7Q9x8HvCTAZYzKKmrP6mrP6mrPzXWdbrtnt82PSqD/nBIattuzXQdE6Wu/qSu/qSu/rzV6sqpm4iIyiXoIyIqV2PQr5zpAiaRuvqTuvqTuvrzlqqrunP0ERFxsBo/0UdERJcEfURE5Y6ZoJd0u6Qdkp6eZP2MTFDeoK5Pl3o2S3pM0rld614o7ZsktYdc14ckvVb2vUnSjV3rlkl6thzLLwy5ruu6anpa0v4y9/CRPl4LJa2TtFXSFknX9Ogz9DHWsK6hj7GGdQ19jDWsa+hjTNIJkp6Q9FSp60s9+hwv6e5yTNarM6PfgXXXl/ZnJV3SdwG2j4kH8BvA+4GnJ1n/UeBBOn9p84PA+tJ+CvBc+XduWZ47xLp+/cD+6Eyivr5r3QvAvBk6Xh8CHujRPgv4O+BfAb8IPAW8d1h1Tej7ceCRIR2vU4H3l+W3A38z8X3PxBhrWNfQx1jDuoY+xprUNRNjrIyZk8ryCJ3pWD84oc8fAbeW5eXA3WX5veUYHQ+cUY7drH72f8x8orf9A2Cqv2U/IxOUT1eX7cfKfgEepzPL1hHX4HhNZgmw3fZztv8JuIvOsZ2Juj4FfHdQ+56K7ZdtbyzLbwDbePP8xkMfY03qmokx1vB4TeaIjbFDqGsoY6yMmd3l6Uh5TLwT5jLgjrJ8D/BhSSrtd9nea/t5OvN+LOln/8dM0Ddw2BOUD8FVdD4RHmDgYUkbJK2YgXp+rfwq+aCks0rbUXG8JL2NTlje29U8lONVfmVeTOdTV7cZHWNT1NVt6GNsmrpmbIxNd7yGPcYkzVJn8qYddD4YTDq+bO8DXgPewQCOV5MZpmIAJF1A53/C87uaz7c9JumdwFpJz5RPvMOwkc7fxtgt6aPA/cCZQ9p3Ex8H/toHz0h2xI+XpJPo/I9/re3XB/nah6NJXTMxxqapa8bGWMP/jkMdY7b3A+dJmgPcJ+ls2z2vVQ1aTZ/oZ3SC8qlIeh/wTeAy268caLc9Vv7dAdxHn7+OHQ7brx/4VdL294ARSfM4Co5XsZwJv1If6eMlaYROONxpe3WPLjMyxhrUNSNjbLq6ZmqMNTlexdDHWHntXcA63nx675+Pi6TjgJOBVxjE8Rr0RYcj+QAWMfnFxd/m4AtlT5T2U4Dn6Vwkm1uWTxliXafROaf26xPaTwTe3rX8GLBsiHX9Ev/yhbklwI/LsTuOzsXEM/iXC2VnDauusv5kOufxTxzW8Srv/VvA16boM/Qx1rCuoY+xhnUNfYw1qWsmxhgwCswpy7OBvwI+NqHP1Rx8MXZVWT6Lgy/GPkefF2OPmVM3kr5L5yr+PEkvAV+kc0EDz+AE5Q3qupHOebavd66rsM+dv073Ljq/vkFn4H/H9veHWNcngc9K2gfsAZa7M6r2Sfoc8BCduyNut71liHUB/A7wsO1/7Nr0iB4vYClwBbC5nEcFuIFOiM7kGGtS10yMsSZ1zcQYa1IXDH+MnQrcIWkWnTMpq2w/IOkmoG17DXAb8G1J2+n8EFpeat4iaRWwFdgHXO3OaaDG8icQIiIqV9M5+oiI6CFBHxFRuQR9RETlEvQREZVL0EdEVC5BHxFRuQR9RETl/j/naLHjHl+a7gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制点图查看两个点的位置\n",
    "plt.plot(A[:,0], A[:, 1], 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果更进一步，我们希望在二维空间中找到一条直线，来拟合这两个点，也就是所谓的构建一个线性回归模型，我们可以设置线性回归方程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> $ y = ax + b $ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "带入（1，2）和（3，4）两个点之后，我们还可以进一步将表达式改写成矩阵表示形式，改写过程如下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pics/张量/线性方程组.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而用矩阵表示线性方程组，则是矩阵的另一种常用用途，接下来，我们就可以通过上述矩阵方程组来求解系数向量x。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先一个基本思路是，如果有个和A矩阵相关的另一个矩阵，假设为$A^{-1}$，可以使得二者相乘之后等于1，也就是$A * A^{-1} = 1$，那么在方程组左右两边同时左乘该矩阵，等式右边的计算结果$A^{-1} * B$就将是x系数向量的取值。而此处的$A^{-1}$就是所谓的A的逆矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逆矩阵定义：    \n",
    "<center> $ 如果存在两个矩阵A、B，并在矩阵乘法运算下，A * B = E（单位矩阵），则我们称A、B互为逆矩阵$  </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上述线性方程组求解场景中，我们已经初步看到了逆矩阵的用途，而一般来说，我们往往会通过伴随矩阵来进行逆矩阵的求解。由于伴随矩阵本身并无其他核心用途，且PyTorch中也未给出伴随矩阵的计算函数（目前），因此我们直接调用inverse函数来进行逆矩阵的计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 当然，并非所有矩阵都有逆矩阵，对于一个矩阵来说，首先必须是方正，其次矩阵的秩不能为零，满足两个条件才能求解逆矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- inverse函数：求解逆矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，根据上述矩阵表达式，从新定义A和B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [3., 1.]])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1.0, 1], [3, 1]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4.])"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([2.0, 4])\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后使用inverse函数进行逆矩阵求解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5000,  0.5000],\n",
       "        [ 1.5000, -0.5000]])"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.inverse(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单试探逆矩阵的基本特性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00, -5.9605e-08],\n",
       "        [-1.1921e-07,  1.0000e+00]])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(torch.inverse(A), A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00, -5.9605e-08],\n",
       "        [-1.1921e-07,  1.0000e+00]])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(A, torch.inverse(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后在方程组左右两边同时左乘$A^{-1}$，求解x      \n",
    "<center> $A^{-1} * A * x= A^{-1} * B $ </center>      \n",
    "<center>$ E * x = A^{-1} * B $  </center>     \n",
    "<center>$ x = A^{-1} * B$ </center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mv(torch.inverse(A), B) # 这里用mv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终得到线性方程为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>$ y = x + 1$ </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.5 矩阵的分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵的分解也是矩阵运算中的常规计算，矩阵分解也有很多种类，常见的例如QR分解、LU分解、特征分解、SVD分解等等等等，虽然大多数情况下，矩阵分解都是在形式上将矩阵拆分成几种特殊矩阵的乘积，但本质上，矩阵的分解是去探索矩阵更深层次的一些属性。本节将主要围绕特征分解和SVD分解展开讲解，更多矩阵分解的运算，我们将在后续课程中逐渐进行介绍。值得一提的是，此前的逆矩阵，其实也可以将其看成是一种矩阵分解的方式，分解之后的等式如下：      \n",
    "<center> $A = A * A^{-1} * A $ </center>      \n",
    "而大多数情况下，矩阵分解都是分解成形如下述形式      \n",
    "<center> $ A = VUD$ </center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.1 特征分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征分解中，矩阵分解形式为：       \n",
    "<center> $ A = Q\\Lambda Q^{-1}$ </center>         \n",
    "其中，Q和$Q^{-1}$互为逆矩阵，并且Q的列就是A的特征值所对应的特征向量，而$\\Lambda$为矩阵A的特征值按照降序排列组成的对角矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.eig函数：特征分解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [7., 8., 9.]])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(1, 10).reshape(3, 3).float()\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里eigenvalues里面的列表的第二个元素是虚部"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.eig(\n",
       "eigenvalues=tensor([[ 1.6117e+01,  0.0000e+00],\n",
       "        [-1.1168e+00,  0.0000e+00],\n",
       "        [ 2.9486e-07,  0.0000e+00]]),\n",
       "eigenvectors=tensor([[-0.2320, -0.7858,  0.4082],\n",
       "        [-0.5253, -0.0868, -0.8165],\n",
       "        [-0.8187,  0.6123,  0.4082]]))"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eig(A, eigenvectors=True)  # 注，此处需要输入参数为True才会返回矩阵的特征向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试其结果是否正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "evector = torch.eig(A, eigenvectors=True).eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalue = torch.eig(A, eigenvectors=True).eigenvalues\n",
    "evalue = torch.tensor([i[0] for i in evalue])\n",
    "evalue = torch.diag(evalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 2.0000, 3.0000],\n",
       "        [4.0000, 5.0000, 6.0000],\n",
       "        [7.0000, 8.0000, 9.0000]])"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(torch.mm(evector, evalue), evector.inverse())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出结果中，eigenvalues表示特征值向量，即A矩阵分解后的Λ矩阵的对角线元素值，并按照又大到小依次排列，eigenvectors表示A矩阵分解后的Q矩阵，此处需要理解特征值，所谓特征值，可简单理解为对应列在矩阵中的信息权重，如果该列能够简单线性变换来表示其他列，则说明该列信息权重较大，反之则较小。特征值概念和秩的概念有点类似，但不完全相同，矩阵的秩表示矩阵列向量的最大线性无关数，而特征值的大小则表示某列向量能多大程度解读矩阵列向量的变异度，即所包含信息量，秩和特征值关系可用下面这个例子来进行解读。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [2., 4.]])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([1, 2, 2, 4]).reshape(2, 2).float()\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matrix_rank(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.eig(\n",
       "eigenvalues=tensor([[0., 0.],\n",
       "        [5., 0.]]),\n",
       "eigenvectors=tensor([]))"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eig(B)   # 返回结果中只有一个特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [2., 4., 6.],\n",
       "        [3., 6., 9.]])"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = torch.tensor([[1, 2, 3], [2, 4, 6], [3, 6, 9]]).float()\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.eig(\n",
       "eigenvalues=tensor([[ 1.4000e+01,  0.0000e+00],\n",
       "        [ 6.2356e-08,  0.0000e+00],\n",
       "        [-2.8243e-07,  0.0000e+00]]),\n",
       "eigenvectors=tensor([]))"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eig(C)                # 只有一个特征的有效值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征值一般用于表示矩阵对应线性方程组解空间以及数据降维，当然，由于特征分解只能作用于方阵，而大多数实际情况下矩阵行列数未必相等，此时要进行类似的操作就需要采用和特征值分解思想类似的奇异值分解（SVD）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.2 奇异值分解（SVD）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "奇异值分解（SVD）来源于代数学中的矩阵分解问题，对于一个方阵来说，我们可以利用矩阵特征值和特征向量的特殊性质（矩阵点乘特征向量等于特征值数乘特征向量），通过求特征值与特征向量来达到矩阵分解的效果     \n",
    "<center> $ A = Q\\Lambda Q^{-1}$ </center>            \n",
    "这里，Q是由特征向量组成的矩阵，而Λ是特征值降序排列构成的一个对角矩阵（对角线上每个值是一个特征值，按降序排列，其他值为0），特征值的数值表示对应的特征的重要性。      \n",
    "在很多情况下，最大的一小部分特征值的和即可以约等于所有特征值的和，而通过矩阵分解的降维就是通过在Q、Λ 中删去那些比较小的特征值及其对应的特征向量，使用一小部分的特征值和特征向量来描述整个矩阵，从而达到降维的效果。      \n",
    "但是，实际问题中大多数矩阵是以奇异矩阵形式，而不是方阵的形式出现的，奇异值分解是特征值分解在奇异矩阵上的推广形式，它将一个维度为m×n的奇异矩阵A分解成三个部分 :      \n",
    "<center> $ A = U\\sum V^{T}$ </center>         \n",
    "其中U、V是两个正交矩阵，其中的每一行（每一列）分别被称为左奇异向量和右奇异向量，他们和∑中对角线上的奇异值相对应，通常情况下我们只需要保留前k个奇异向量和奇异值即可，其中U是m×k矩阵，V是n×k矩阵，∑是k×k的方阵，从而达到减少存储空间的效果，即      \n",
    "<center> $ A_{m*n} = U_{m*m}\\sum_{m*n}V^{T}_{n*n}\\approx U_{m*k}\\sum_{k*k}V^{T}_{k*n}$ </center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- svd奇异值分解函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [2., 4., 6.],\n",
       "        [3., 6., 9.]])"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.svd(\n",
       "U=tensor([[-2.6726e-01,  9.6362e-01, -3.7767e-08],\n",
       "        [-5.3452e-01, -1.4825e-01, -8.3205e-01],\n",
       "        [-8.0178e-01, -2.2237e-01,  5.5470e-01]]),\n",
       "S=tensor([1.4000e+01, 4.2751e-08, 1.6397e-15]),\n",
       "V=tensor([[-0.2673, -0.9636,  0.0000],\n",
       "        [-0.5345,  0.1482, -0.8321],\n",
       "        [-0.8018,  0.2224,  0.5547]]))"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.svd(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "CU, CS, CV = torch.svd(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证SVD分解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4000e+01, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 4.2751e-08, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 1.6397e-15]])"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(CS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 2.0000, 3.0000],\n",
       "        [2.0000, 4.0000, 6.0000],\n",
       "        [3.0000, 6.0000, 9.0000]])"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(torch.mm(CU, torch.diag(CS)), CV.t())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够看出，上述输出完整还原了C矩阵，此时我们可根据svd输出结果对C进行降维，此时C可只保留第一列（后面的奇异值过小），即k=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2673],\n",
       "        [-0.5345],\n",
       "        [-0.8018]])"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U1 = CU[:, 0].reshape(3, 1)  # U的第一列\n",
    "U1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.0000)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C1 = CS[0]                           # C的第一个值\n",
    "C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2673, -0.5345, -0.8018]])"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V1 = CV[:, 0].reshape(1, 3)           # V的第一行\n",
    "V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 2.0000, 3.0000],\n",
       "        [2.0000, 4.0000, 6.0000],\n",
       "        [3.0000, 6.0000, 9.0000]])"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm((U1 * C1), V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时输出的Cd矩阵已经和原矩阵C高度相似了，损失信息在R的计算中基本可以忽略不计，经过SVD分解，矩阵的信息能够被压缩至更小的空间内进行存储，从而为PCA（主成分分析）、LSI（潜在语义索引）等算法做好了数学工具层面的铺垫。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**本节选读内容**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 另外，我们需要知道的是，除了利用逆矩阵求解线性方程组系数外，比较通用的方法是使用最小二乘法进行求解："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.lstsq：最小二乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最小二乘法是最通用的线性方程拟合求解工具，我们可以利用最小二乘法的直接计算拟合直线的系数最优解。当然，本节仅介绍最小二乘法的函数调用，下节在介绍目标函数和优化手段时，还将进一步介绍最小二乘法的数学原理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [2., 4.]])"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([[1, 2], [2, 4]]).float()\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [4., 5.]])"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2], [4, 5]]).float()     # 秩的计算要求浮点型张量\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, q = torch.lstsq(B, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3333, -0.6667],\n",
       "        [ 0.6667,  1.3333]])"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.1231, -5.3358],\n",
       "        [ 0.7808, -0.7276]])"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们发现，最小二乘法返回了两个结果，分别是x的系数和QR分解后的QR矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- solve函数与LU分解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.solve(\n",
       "solution=tensor([[-0.3333, -0.6667],\n",
       "        [ 0.6667,  1.3333]]),\n",
       "LU=tensor([[4.0000, 5.0000],\n",
       "        [0.2500, 0.7500]]))"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.solve(B, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LU分解函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4.0000, 5.0000],\n",
       "         [0.2500, 0.7500]]),\n",
       " tensor([2, 2], dtype=torch.int32))"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.lu(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.6 补充：线性回归三种方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.6.1 使用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  6.8723],\n",
      "        [  4.1915],\n",
      "        [-23.8936]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([[1., 2., 1.], [2., 4., 1.], [3., 5., 1.], [4., 2., 1.], [4., 4., 1.]])\n",
    "y = torch.tensor([-12., 13., 15., 14., 18.])\n",
    "\n",
    "wr, _ = torch.lstsq(y, x)   # 返回2个值\n",
    "w = wr[: 3]    # 因为x大小为(5,3)所以，取wr的前3个元素为所得权重\n",
    "print(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  6.8723],\n",
       "        [  4.1915],\n",
       "        [-23.8936],\n",
       "        [  8.0765],\n",
       "        [  1.9141]])"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3]), torch.Size([1, 3]))"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.6290],\n",
       "        [18.8843],\n",
       "        [29.9482],\n",
       "        [24.2460],\n",
       "        [32.6290]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(x, w.t())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.6.2 使用优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0, loss = 211.60000610351562, W = [0.0, 0.0, 0.0]\n",
      "step = 1000, loss = 102.77001953125, W = [0.8611137270927429, 0.8414422273635864, 0.805219829082489]\n",
      "step = 2000, loss = 74.26525115966797, W = [1.445191740989685, 1.335961937904358, 1.1051279306411743]\n",
      "step = 3000, loss = 68.27497863769531, W = [1.8296148777008057, 1.5062566995620728, 0.7510571479797363]\n",
      "step = 4000, loss = 63.64544677734375, W = [2.185516357421875, 1.5048502683639526, -0.039954569190740585]\n",
      "step = 5000, loss = 58.63645553588867, W = [2.6053810119628906, 1.4383796453475952, -0.9721460342407227]\n",
      "step = 6000, loss = 53.781166076660156, W = [3.076796293258667, 1.3319281339645386, -1.9445074796676636]\n",
      "step = 7000, loss = 49.4732666015625, W = [3.552100419998169, 1.2203000783920288, -2.9248034954071045]\n",
      "step = 8000, loss = 45.816139221191406, W = [3.969752550125122, 1.1532857418060303, -3.9039103984832764]\n",
      "step = 9000, loss = 42.652748107910156, W = [4.279463291168213, 1.1704198122024536, -4.878206253051758]\n",
      "step = 10000, loss = 39.77521514892578, W = [4.480603218078613, 1.271019458770752, -5.844773292541504]\n",
      "step = 11000, loss = 37.08690643310547, W = [4.625091552734375, 1.4135740995407104, -6.8029303550720215]\n",
      "step = 12000, loss = 34.56438064575195, W = [4.755249500274658, 1.565032958984375, -7.7541422843933105]\n",
      "step = 13000, loss = 32.19972229003906, W = [4.881856441497803, 1.7174465656280518, -8.699872016906738]\n",
      "step = 14000, loss = 29.988739013671875, W = [5.006570816040039, 1.8698391914367676, -9.640814781188965]\n",
      "step = 15000, loss = 27.928882598876953, W = [5.129983901977539, 2.021883010864258, -10.577141761779785]\n",
      "step = 16000, loss = 26.01841163635254, W = [5.252359867095947, 2.173382520675659, -11.508679389953613]\n",
      "step = 17000, loss = 24.25580406188965, W = [5.37382698059082, 2.324178695678711, -12.435089111328125]\n",
      "step = 18000, loss = 22.639862060546875, W = [5.494397163391113, 2.4741153717041016, -13.355731010437012]\n",
      "step = 19000, loss = 21.169158935546875, W = [5.614033222198486, 2.623039960861206, -14.269880294799805]\n",
      "step = 20000, loss = 19.842281341552734, W = [5.732634544372559, 2.7707676887512207, -15.176507949829102]\n",
      "step = 21000, loss = 18.657634735107422, W = [5.850046157836914, 2.917067050933838, -16.074275970458984]\n",
      "step = 22000, loss = 17.613204956054688, W = [5.96606969833374, 3.0616695880889893, -16.961563110351562]\n",
      "step = 23000, loss = 16.70669937133789, W = [6.080419063568115, 3.204202890396118, -17.836130142211914]\n",
      "step = 24000, loss = 15.935266494750977, W = [6.192723751068115, 3.344191074371338, -18.69499969482422]\n",
      "step = 25000, loss = 15.295303344726562, W = [6.3023905754089355, 3.4808998107910156, -19.53406524658203]\n",
      "step = 26000, loss = 14.782068252563477, W = [6.408851623535156, 3.6136090755462646, -20.34765625]\n",
      "step = 27000, loss = 14.38933277130127, W = [6.510801315307617, 3.740706443786621, -21.127376556396484]\n",
      "step = 28000, loss = 14.108491897583008, W = [6.606583595275879, 3.8601436614990234, -21.86072540283203]\n",
      "step = 29000, loss = 13.927469253540039, W = [6.6938862800598145, 3.9689838886260986, -22.5283145904541]\n",
      "step = 30000, loss = 13.828953742980957, W = [6.768626689910889, 4.062172889709473, -23.100189208984375]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim\n",
    "\n",
    "x = torch.tensor([[1., 2., 1.], [2., 4., 1.], [3., 5., 1.], [4., 2., 1.], [4., 4., 1.]])\n",
    "y = torch.tensor([-12., 13., 15., 14., 18.])\n",
    "w = torch.zeros(3, requires_grad=True)\n",
    "\n",
    "L = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam([w, ],)\n",
    "\n",
    "for step in range(30001):\n",
    "    if step:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    pred = torch.mv(x, w)\n",
    "    loss = L(pred, y)\n",
    "    if step % 1000 == 0:\n",
    "        print('step = {}, loss = {}, W = {}'.format(step, loss, w.tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.6.3 使用torch.nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0, loss = 234.15921020507812, w = [[-0.025971829891204834, -0.3131893277168274, 0.468066930770874]], b= -0.1727270483970642\n",
      "step = 1000, loss = 108.45240783691406, w = [[0.8268170952796936, 0.5213714241981506, 1.2636305093765259]], b= 0.6228362321853638\n",
      "step = 2000, loss = 79.05266571044922, w = [[1.3922544717788696, 1.0043445825576782, 1.539814829826355]], b= 0.8990193605422974\n",
      "step = 3000, loss = 71.98594665527344, w = [[1.7823017835617065, 1.2073900699615479, 1.213461995124817]], b= 0.5726637244224548\n",
      "step = 4000, loss = 64.98625946044922, w = [[2.187448740005493, 1.3202823400497437, 0.5124436616897583]], b= -0.12835487723350525\n",
      "step = 5000, loss = 57.197120666503906, w = [[2.6739959716796875, 1.4280478954315186, -0.3521043062210083]], b= -0.9929031133651733\n",
      "step = 6000, loss = 49.5626220703125, w = [[3.211785078048706, 1.5261608362197876, -1.287392258644104]], b= -1.9281911849975586\n",
      "step = 7000, loss = 42.69411087036133, w = [[3.7578365802764893, 1.6186585426330566, -2.239631175994873]], b= -2.880434036254883\n",
      "step = 8000, loss = 36.80388641357422, w = [[4.269256114959717, 1.729954719543457, -3.185861110687256]], b= -3.8266639709472656\n",
      "step = 9000, loss = 31.847164154052734, w = [[4.708168983459473, 1.8877356052398682, -4.118001461029053]], b= -4.7588043212890625\n",
      "step = 10000, loss = 27.67670249938965, w = [[5.058448314666748, 2.1029908657073975, -5.0313334465026855]], b= -5.672135829925537\n",
      "step = 11000, loss = 24.1680908203125, w = [[5.339318752288818, 2.3590853214263916, -5.9221014976501465]], b= -6.562903881072998\n",
      "step = 12000, loss = 21.25197410583496, w = [[5.585055351257324, 2.6283164024353027, -6.787974834442139]], b= -7.42877721786499\n",
      "step = 13000, loss = 18.885883331298828, w = [[5.81389045715332, 2.895444393157959, -7.6264567375183105]], b= -8.267260551452637\n",
      "step = 14000, loss = 17.034635543823242, w = [[6.030049800872803, 3.1548333168029785, -8.432607650756836]], b= -9.073407173156738\n",
      "step = 15000, loss = 15.661699295043945, w = [[6.233185768127441, 3.402256727218628, -9.19758129119873]], b= -9.838380813598633\n",
      "step = 16000, loss = 14.722561836242676, w = [[6.420460224151611, 3.6323208808898926, -9.906816482543945]], b= -10.547616004943848\n",
      "step = 17000, loss = 14.157549858093262, w = [[6.586313724517822, 3.837082862854004, -10.536993980407715]], b= -11.1777925491333\n",
      "step = 18000, loss = 13.883906364440918, w = [[6.7216973304748535, 4.0046916007995605, -11.052353858947754]], b= -11.69315242767334\n",
      "step = 19000, loss = 13.7940092086792, w = [[6.814924716949463, 4.120269775390625, -11.407567024230957]], b= -12.048365592956543\n",
      "step = 20000, loss = 13.779424667358398, w = [[6.860092639923096, 4.176296234130859, -11.57972240447998]], b= -12.220520973205566\n",
      "step = 21000, loss = 13.778727531433105, w = [[6.871392726898193, 4.190314292907715, -11.622797966003418]], b= -12.263596534729004\n",
      "step = 22000, loss = 13.778717041015625, w = [[6.872317314147949, 4.191461086273193, -11.626320838928223]], b= -12.267119407653809\n",
      "step = 23000, loss = 13.778724670410156, w = [[6.87232780456543, 4.191474437713623, -11.626358032226562]], b= -12.267158508300781\n",
      "step = 24000, loss = 13.778721809387207, w = [[6.872341632843018, 4.191493511199951, -11.62640380859375]], b= -12.267203330993652\n",
      "step = 25000, loss = 13.778718948364258, w = [[6.872337818145752, 4.191486835479736, -11.626408576965332]], b= -12.267207145690918\n",
      "step = 26000, loss = 13.778726577758789, w = [[6.872342109680176, 4.191491603851318, -11.626410484313965]], b= -12.26720905303955\n",
      "step = 27000, loss = 13.778727531433105, w = [[6.872381210327148, 4.191532135009766, -11.626405715942383]], b= -12.267206192016602\n",
      "step = 28000, loss = 13.778721809387207, w = [[6.872350692749023, 4.191500663757324, -11.626407623291016]], b= -12.267208099365234\n",
      "step = 29000, loss = 13.778722763061523, w = [[6.8722686767578125, 4.191418647766113, -11.626420021057129]], b= -12.267223358154297\n",
      "step = 30000, loss = 13.778724670410156, w = [[6.872353553771973, 4.191502094268799, -11.626402854919434]], b= -12.267207145690918\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim\n",
    "\n",
    "x = torch.tensor([[1., 2., 1.], [2., 4., 1.], [3., 5., 1.], [4., 2., 1.], [4., 4., 1.]])\n",
    "y = torch.tensor([-12., 13., 15., 14., 18.]).reshape(-1, 1)\n",
    "\n",
    "fc = torch.nn.Linear(3, 1)\n",
    "L = torch.nn.MSELoss()\n",
    "optimiter = torch.optim.Adam(fc.parameters())\n",
    "w, b = fc.parameters()\n",
    "\n",
    "for step in range(30001):\n",
    "    if step:\n",
    "        optimiter.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiter.step()\n",
    "    pred = fc(x)\n",
    "    loss = L(pred, y)\n",
    "    if step % 1000 == 0:\n",
    "        print('step = {}, loss = {}, w = {}, b= {}'.format(step, loss, w.tolist(), b.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
